{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "tags": []
      },
      "source": [
        "var IntermediateFolderPath = \"abfss://<container_name>@<storage_account_name>.dfs.core.windows.net/intermediate_output/\"\n",
        "var StorageAccountName = \"<storage_account_name>\"\n",
        "var StorageAccountAccessKey = \"<storage_account_access_key>\"\n",
        "\n",
        "var DatabasePrefix = \"\"\n",
        "var TablePrefix = \"\"\n",
        "var IgnoreIfExists = false\n",
        "var OverrideIfExists = false\n",
        "\n",
        "var LocationPrefixMappings:Map[String, String] = Map(\"dbfs:/user/hive/warehouse\"->\"abfss://<container_name>@<storage_account_name>.dfs.core.windows.net/catalog/hive/warehouse\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "spark.conf.set(\n",
        "  \"fs.azure.account.key.\" + StorageAccountName + \".dfs.core.windows.net\",\n",
        "  StorageAccountAccessKey\n",
        ")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "database_description_item"
            ],
            "values": [
              "database_description_item"
            ],
            "yLabel": "database_description_item",
            "xLabel": "database_description_item",
            "aggregation": "COUNT",
            "aggByBackend": false
          },
          "aggData": "{\"database_description_item\":{\"Database Name\":1,\"Description\":1,\"Location\":1}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": true
        }
      },
      "source": [
        "import java.net.URI\n",
        "import java.util.Calendar\n",
        "\n",
        "import scala.collection.mutable.{ListBuffer, Map}\n",
        "import org.apache.spark.sql._\n",
        "import org.apache.spark.sql.types.{ObjectType, _}\n",
        "import org.apache.spark.sql.catalyst._\n",
        "import org.apache.spark.sql.catalyst.catalog._\n",
        "import org.json4s._\n",
        "import org.json4s.JsonAST.JString\n",
        "import org.json4s.jackson.Serialization\n",
        "\n",
        "\n",
        "var locationPrefixMappingList = Map(LocationPrefixMappings.toSeq: _*).toList.sortBy(pair => pair._1).reverse\n",
        "\n",
        "object ImportMetadata {\n",
        "\n",
        "  val spark = SparkSession.builder().getOrCreate()\n",
        "\n",
        "\n",
        "  // define custom json serializer for java.net.URI\n",
        "  case object URISerializer extends CustomSerializer[URI](format => ( {\n",
        "    case JString(uri) => new URI(uri)\n",
        "  }, {\n",
        "    case uri: URI => JString(uri.toString())\n",
        "  }))\n",
        "\n",
        "  // define custom json serializer for  org.apache.spark.sql.types.StructType\n",
        "  case object SturctTypeSerializer extends CustomSerializer[StructType](format => ( {\n",
        "    case JString(structType)  => DataType.fromJson(structType).asInstanceOf[StructType]\n",
        "  }, {\n",
        "    case structType: StructType => JString(structType.json)\n",
        "  }))\n",
        "\n",
        "\n",
        "  // define formats for org.json4s.jackson.Serialization\n",
        "  implicit val formats = DefaultFormats + URISerializer + SturctTypeSerializer// define custom json serializer for java.net.URI\n",
        "\n",
        "\n",
        "  case class CatalogPartitions(database: String, table: String, tablePartitons: Seq[CatalogTablePartition])\n",
        "\n",
        "  case class CatalogTables(database: String, tables: Seq[CatalogTable])\n",
        "\n",
        "  def ConvertLocation(location: String) : String = {\n",
        "    var locationMapping = locationPrefixMappingList.find(mapping => {location.startsWith(mapping._1)})\n",
        "\n",
        "    if (locationMapping != None) {\n",
        "      return location.replaceFirst(locationMapping.get._1, locationMapping.get._2)\n",
        "    }\n",
        "\n",
        "    return location;\n",
        "  }\n",
        "\n",
        "  def ConvertCatalogDatabase(databsae: CatalogDatabase) : CatalogDatabase = {\n",
        "    //class CatalogDatabase(\n",
        "    // name: String,\n",
        "    // description: String,\n",
        "    // locationUri: URI,\n",
        "    // properties: Map[String, String])\n",
        "    // extends Product\n",
        "    var convertedDatabsae  = new CatalogDatabase(\n",
        "      DatabasePrefix + databsae.name,\n",
        "      databsae.description,\n",
        "      new URI(ConvertLocation(databsae.locationUri.toString())), //databsae.locationUri\n",
        "      databsae.properties)\n",
        "\n",
        "    return convertedDatabsae;\n",
        "  }\n",
        "\n",
        "  def ConvertCatalogStorageFormat(format : CatalogStorageFormat) : CatalogStorageFormat = {\n",
        "\n",
        "    var formatlocation: Option[URI] = None\n",
        "    if (format.locationUri != None) {\n",
        "      formatlocation = Some(new URI(ConvertLocation(format.locationUri.get.toString())))\n",
        "    }\n",
        "\n",
        "    //class CatalogStorageFormat(\n",
        "    // locationUri: Option[URI],\n",
        "    // inputFormat: Option[String],\n",
        "    // outputFormat: Option[String],\n",
        "    // serde: Option[String],\n",
        "    // compressed: Boolean,\n",
        "    // properties: Map[String, String])\n",
        "    //  extends Product\n",
        "    var convertedStorageFormat = new CatalogStorageFormat(\n",
        "      formatlocation,\n",
        "      format.inputFormat,\n",
        "      format.outputFormat,\n",
        "      format.serde,\n",
        "      format.compressed,\n",
        "      format.properties\n",
        "    )\n",
        "\n",
        "    return  convertedStorageFormat;\n",
        "  }\n",
        "\n",
        "  def ConvertCatalogTable(table: CatalogTable) : CatalogTable = {\n",
        "\n",
        "    var dbName = Some(DatabasePrefix + table.identifier.database.get);\n",
        "    var tblName = TablePrefix + table.identifier.table;\n",
        "\n",
        "    //class CatalogTable(\n",
        "    // identifier: TableIdentifier,\n",
        "    // tableType: CatalogTableType,\n",
        "    // storage: CatalogStorageFormat,\n",
        "    // schema: StructType,\n",
        "    // provider: Option[String] = None,\n",
        "    // partitionColumnNames: scala.Seq[String] = Seq.empty,\n",
        "    // bucketSpec: Option[BucketSpec] = None,\n",
        "    // owner: String = \"\",\n",
        "    // createTime: Long = System.currentTimeMi...,\n",
        "    // lastAccessTime: Long = -1,\n",
        "    // createVersion: String = \"\",\n",
        "    // properties: Map[String, String] = Map.empty,\n",
        "    // stats: Option[CatalogStatistics] = None,\n",
        "    // viewText: Option[String] = None,\n",
        "    // comment: Option[String] = None,\n",
        "    // unsupportedFeatures: scala.Seq[String] = Seq.empty,\n",
        "    // tracksPartitionsInCatalog: Boolean = false,\n",
        "    // schemaPreservesCase: Boolean = true,\n",
        "    // ignoredProperties: Map[String, String] = Map.empty)\n",
        "    // extends Product\n",
        "    var convertedTable = new CatalogTable(\n",
        "      //class TableIdentifier(table: String,database: Option[String])\n",
        "      // extends IdentifierWithDatabase\n",
        "      new TableIdentifier(tblName, dbName),\n",
        "      table.tableType,\n",
        "      ConvertCatalogStorageFormat(table.storage),\n",
        "      table.schema,\n",
        "      table.provider,\n",
        "      table.partitionColumnNames,\n",
        "      table.bucketSpec,\n",
        "      table.owner,\n",
        "      table.createTime,\n",
        "      table.lastAccessTime,\n",
        "      table.createVersion,\n",
        "      table.properties,\n",
        "      table.stats,\n",
        "      table.viewText,\n",
        "      table.comment,\n",
        "      table.unsupportedFeatures,\n",
        "      table.tracksPartitionsInCatalog,\n",
        "      table.schemaPreservesCase,\n",
        "      table.ignoredProperties)\n",
        "\n",
        "    return convertedTable;\n",
        "  }\n",
        "\n",
        "\n",
        "  def ConvertCatalogTablePartition(partition : CatalogTablePartition) : CatalogTablePartition = {\n",
        "    //class CatalogTablePartition(\n",
        "    // spec: CatalogTypes.TablePartitionSpec,\n",
        "    // storage: CatalogStorageFormat,\n",
        "    // parameters: Map[String, String] = Map.empty,\n",
        "    // createTime: Long = System.currentTimeMi...,\n",
        "    // lastAccessTime: Long = -1,\n",
        "    // stats: Option[CatalogStatistics] = None)\n",
        "    // extends Product\n",
        "    var convertedPartition = new CatalogTablePartition(\n",
        "      partition.spec,\n",
        "      ConvertCatalogStorageFormat(partition.storage),\n",
        "      partition.parameters,\n",
        "      partition.createTime,\n",
        "      partition.lastAccessTime,\n",
        "      partition.stats\n",
        "    );\n",
        "\n",
        "    return convertedPartition;\n",
        "  }\n",
        "\n",
        "  val MaxRetryCount = 3;\n",
        "\n",
        "  def RetriableFunc(func: () => Unit, retryCount: Int = 0): Unit = {\n",
        "    try {\n",
        "      func()\n",
        "    } catch {\n",
        "      case e:Exception => {\n",
        "        if (retryCount < MaxRetryCount){\n",
        "          RetriableFunc(func, retryCount + 1)\n",
        "        } else {\n",
        "          throw e\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  def RetriableQueryFunc(func: () => Object, retryCount: Int = 0): Object = {\n",
        "    try {\n",
        "      func()\n",
        "    } catch {\n",
        "      case e:Exception => {\n",
        "        if (retryCount < MaxRetryCount){\n",
        "          RetriableQueryFunc(func, retryCount + 1)\n",
        "        } else {\n",
        "          throw e\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        " \n",
        "  def CreateDatabases(dataPath: String) = {\n",
        "\n",
        "    println(\"Start to create databases \" + Calendar.getInstance().getTime())\n",
        "\n",
        "    val ds = spark.read.format(\"text\").load(dataPath)\n",
        "\n",
        "    var createdCount = 0;\n",
        "    var existsDbs = spark.sharedState.externalCatalog.listDatabases()\n",
        "    var data = ds.collect()\n",
        "    var total = data.size\n",
        "\n",
        "    data.foreach(row => {\n",
        "      var jsonString = row.getString(0)\n",
        "      var newDb = ConvertCatalogDatabase(Serialization.read[CatalogDatabase](jsonString))\n",
        "\n",
        "      var exists = existsDbs.contains(newDb.name)\n",
        "      if (exists && !IgnoreIfExists && !OverrideIfExists) {\n",
        "\n",
        "        println(createdCount + \"/\" + total + \" databases created. \" + Calendar.getInstance().getTime())\n",
        "        println(\"Database \" + newDb.name + \" already exists\")\n",
        "\n",
        "        throw new Exception(\"Database \" + newDb.name + \" already exists\")\n",
        "      } else if (!exists || OverrideIfExists) {\n",
        "        CreateDatabase(newDb)\n",
        "      }\n",
        "\n",
        "      createdCount+=1;\n",
        "\n",
        "      if (createdCount%100 == 0) {\n",
        "        println(createdCount + \"/\" + total + \" databases created\" + Calendar.getInstance().getTime())\n",
        "      }\n",
        "    });\n",
        "\n",
        "    println(\"Databases Created completed. Total \" + createdCount + \" database created. \" + Calendar.getInstance().getTime())\n",
        "  }\n",
        "\n",
        "  def CreateDatabase(db:CatalogDatabase) = {\n",
        "    // Drop exists db if overrideIfExists\n",
        "    if (OverrideIfExists) {\n",
        "      RetriableFunc(() =>  {\n",
        "        spark.sharedState.externalCatalog.dropDatabase(db.name, true, true)\n",
        "      })\n",
        "    }\n",
        "\n",
        "    // Create db\n",
        "    RetriableFunc(() => {\n",
        "      spark.sharedState.externalCatalog.createDatabase(db, IgnoreIfExists)\n",
        "    })\n",
        "  }\n",
        "\n",
        "  def CreateTables(dataPath: String) = {\n",
        "    println(\"Start to create tables \" + Calendar.getInstance().getTime())\n",
        "\n",
        "    val ds = spark.read.format(\"text\").load(dataPath);\n",
        "\n",
        "    var createdCount = 0;\n",
        "    ds.collect().foreach(row => {\n",
        "      var jsonString = row.getString(0)\n",
        "      var tables = Serialization.read[CatalogTables](jsonString);\n",
        "\n",
        "      var existsTables = spark.sharedState.externalCatalog.listTables(DatabasePrefix + tables.database)\n",
        "      var perTables = tables.tables.toParArray\n",
        "\n",
        "      perTables.foreach(table => {\n",
        "        var newTable = ConvertCatalogTable(table)\n",
        "        var exists = existsTables.contains(newTable.identifier.table)\n",
        "        if (exists && !IgnoreIfExists) {\n",
        "\n",
        "          println(createdCount + \" tables created. \" + Calendar.getInstance().getTime())\n",
        "          println(\"Table \" + newTable.identifier.database + \".\" + newTable.identifier.table + \" already exists\")\n",
        "\n",
        "          throw new Exception(\"Table \" + newTable.identifier.database + \".\" + newTable.identifier.table + \" already exists\")\n",
        "        } else if (!exists) {\n",
        "          CreateTable(newTable)\n",
        "        }\n",
        "\n",
        "        createdCount += 1;\n",
        "      })\n",
        "\n",
        "      println(createdCount + \" tables created\" + Calendar.getInstance().getTime())\n",
        "    })\n",
        "\n",
        "    println(\"Tables Created completed. Total \" + createdCount + \" table created. \" + Calendar.getInstance().getTime())\n",
        "  }\n",
        "\n",
        "  def CreateTable(table:CatalogTable) = {\n",
        "    // Create table\n",
        "    RetriableFunc(() => {\n",
        "      spark.sharedState.externalCatalog.createTable(table, IgnoreIfExists)\n",
        "    })\n",
        "  }\n",
        "\n",
        "  def CreatePartitions(dataPath: String) = {\n",
        "    println(\"Start to create partitions \" + Calendar.getInstance().getTime())\n",
        "\n",
        "    val ds = spark.read.format(\"text\").load(dataPath);\n",
        "\n",
        "    var createdCount = 0;\n",
        "    ds.collect().foreach(row => {\n",
        "      var jsonString = row.getString(0)\n",
        "      var parts = Serialization.read[CatalogPartitions](jsonString);\n",
        "\n",
        "      var catalogTablePartitions = new ListBuffer[CatalogTablePartition]()\n",
        "      parts.tablePartitons.foreach( part => {\n",
        "        catalogTablePartitions += ConvertCatalogTablePartition(part)\n",
        "      })\n",
        "\n",
        "      RetriableFunc(() => {\n",
        "        spark.sharedState.externalCatalog.createPartitions(DatabasePrefix + parts.database, TablePrefix + parts.table, catalogTablePartitions, IgnoreIfExists)\n",
        "      })\n",
        "\n",
        "      createdCount+=catalogTablePartitions.size;\n",
        "      println(createdCount +  \" partitions created\" + Calendar.getInstance().getTime())\n",
        "    });\n",
        "\n",
        "    println(\"Partition Created completed. Total \" + createdCount + \" partition created. \" + Calendar.getInstance().getTime())\n",
        "  }\n",
        "\n",
        "  def ImportCatalogObjectsFromFile(inputPath: String) = {\n",
        "\n",
        "    val dbsPath = inputPath + \"databases\";\n",
        "    val tablesPath = inputPath + \"tables\";\n",
        "    val partPath = inputPath + \"partitions\";\n",
        "\n",
        "    CreateDatabases(dbsPath)\n",
        "    CreateTables(tablesPath)\n",
        "    CreatePartitions(partPath)\n",
        "  }\n",
        "}"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "ImportMetadata.CreateDatabases(IntermediateFolderPath + \"databases\")    "
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "ImportMetadata.CreateTables(IntermediateFolderPath + \"tables\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "ImportMetadata.CreatePartitions(IntermediateFolderPath + \"partitions\")"
      ],
      "attachments": {}
    }
  ]
}